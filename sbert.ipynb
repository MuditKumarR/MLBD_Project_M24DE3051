{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "35c92d18-5930-49e1-b6ba-d7237f960a16",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|████████████████████████████████████| 1/1 [00:00<00:00, 16.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            Job Title                   Company  \\\n",
      "1111523  Digital Marketing Specialist  Associated British Foods   \n",
      "1161255  Digital Marketing Specialist  Associated British Foods   \n",
      "397716   Digital Marketing Specialist  Associated British Foods   \n",
      "1529428  Digital Marketing Specialist  Associated British Foods   \n",
      "513603   Digital Marketing Specialist  Associated British Foods   \n",
      "\n",
      "                         Role  location  similarity  \n",
      "1111523  Social Media Manager     Dakar     0.62881  \n",
      "1161255  Social Media Manager    Bangui     0.62881  \n",
      "397716   Social Media Manager    Madrid     0.62881  \n",
      "1529428  Social Media Manager   Mbabane     0.62881  \n",
      "513603   Social Media Manager  Funafuti     0.62881  \n"
     ]
    }
   ],
   "source": [
    "# Resume-to-Job Recommender System using SBERT\n",
    "\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "import time\n",
    "import logging\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "try:\n",
    "    import fitz  # PyMuPDF\n",
    "except ImportError:\n",
    "    raise ImportError(\"Install PyMuPDF with: pip install PyMuPDF\")\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(message)s', filename='job_recommender_sbert.log')\n",
    "logger = logging.getLogger()\n",
    "\n",
    "# --- Utility Functions ---\n",
    "def extract_resume_text(pdf_path):\n",
    "    try:\n",
    "        doc = fitz.open(pdf_path)\n",
    "        return \" \".join([page.get_text() for page in doc])\n",
    "    except Exception as e:\n",
    "        logger.error(f\"PDF extraction failed: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "def clean_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\n', ' ', text)\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    return re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "def weight_text(text, weight):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    weighted = text * int(weight)\n",
    "    frac = weight - int(weight)\n",
    "    if frac:\n",
    "        words = text.split()\n",
    "        weighted += \" \" + \" \".join(words[:int(len(words) * frac)])\n",
    "    return weighted\n",
    "\n",
    "# --- SBERT Caching ---\n",
    "def build_or_load_sbert_embeddings(jobs_df, cache_dir=\"./sbert_cache\"):\n",
    "    os.makedirs(cache_dir, exist_ok=True)\n",
    "    vec_path = os.path.join(cache_dir, \"job_embeddings.npy\")\n",
    "    model_path = os.path.join(cache_dir, \"sbert_model.pkl\")\n",
    "\n",
    "    if os.path.exists(vec_path) and os.path.exists(model_path):\n",
    "        logger.info(\"Loading cached SBERT embeddings...\")\n",
    "        return np.load(vec_path), joblib.load(model_path), jobs_df\n",
    "\n",
    "    weights = {'Job Title': 3.0, 'Role': 2.5, 'skills': 2.0, 'Job Description': 1.0, 'Company': 0.8}\n",
    "    jobs_df['Weighted_Text'] = \"\"\n",
    "    for field, weight in weights.items():\n",
    "        if field in jobs_df.columns:\n",
    "            jobs_df['Weighted_Text'] += jobs_df[field].fillna('').apply(lambda x: weight_text(str(x), weight) + \" \")\n",
    "    jobs_df['Weighted_Text_Clean'] = jobs_df['Weighted_Text'].apply(clean_text)\n",
    "\n",
    "    model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    job_embeddings = model.encode(jobs_df['Weighted_Text_Clean'].tolist(), convert_to_numpy=True, show_progress_bar=True)\n",
    "\n",
    "    np.save(vec_path, job_embeddings)\n",
    "    joblib.dump(model, model_path)\n",
    "    return job_embeddings, model, jobs_df\n",
    "\n",
    "# --- Similarity Matching ---\n",
    "def match_resume_sbert(resume_text, job_embeddings, model, jobs_df, top_n=10):\n",
    "    resume_clean = clean_text(resume_text)\n",
    "    resume_embedding = model.encode(resume_clean, convert_to_tensor=True)\n",
    "    job_embeddings_tensor = torch.tensor(job_embeddings).to(resume_embedding.device)\n",
    "\n",
    "    similarities = util.pytorch_cos_sim(resume_embedding, job_embeddings_tensor)[0].cpu().numpy()\n",
    "    jobs_df = jobs_df.copy()\n",
    "    jobs_df['similarity'] = similarities\n",
    "    return jobs_df.sort_values('similarity', ascending=False).head(top_n)\n",
    "\n",
    "# --- Main Recommender ---\n",
    "def run_job_recommender_sbert(resume_path, jobs_csv_path, top_n=10):\n",
    "    start = time.time()\n",
    "    logger.info(\"Loading data...\")\n",
    "    jobs_df = pd.read_csv(jobs_csv_path)\n",
    "    resume_text = extract_resume_text(resume_path)\n",
    "    if not resume_text:\n",
    "        logger.warning(\"Resume extraction failed.\")\n",
    "        return pd.DataFrame({'Message': ['Resume extraction failed']})\n",
    "\n",
    "    job_embeddings, model, processed_df = build_or_load_sbert_embeddings(jobs_df)\n",
    "    results = match_resume_sbert(resume_text, job_embeddings, model, processed_df, top_n)\n",
    "    logger.info(f\"Total execution time: {time.time() - start:.2f}s\")\n",
    "    return results\n",
    "\n",
    "# --- Example Execution ---\n",
    "if __name__ == \"__main__\":\n",
    "    resume_path = 'sampleresume.pdf'\n",
    "    jobs_csv_path = 'job_descriptions.csv'\n",
    "    top_matches = run_job_recommender_sbert(resume_path, jobs_csv_path, top_n=5)\n",
    "    print(top_matches[['Job Title', 'Company', 'Role', 'location', 'similarity']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6037318e-45e6-41a4-b787-748e8644b05a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
