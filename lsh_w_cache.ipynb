{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ff824e0-eac2-47ad-a639-3034b8e19126",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to Snowflake successfully.\n"
     ]
    }
   ],
   "source": [
    "# Install the Snowflake connector if not already installed:\n",
    "# !pip install snowflake-connector-python\n",
    "\n",
    "import snowflake.connector\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Connect to Snowflake using given credentials\n",
    "conn = snowflake.connector.connect(\n",
    "    user='MUDIT',\n",
    "    password='Testing@123123',\n",
    "    account='BCEMHHI-LB94703',\n",
    "    warehouse='COMPUTE_WH',\n",
    "    database='JOB_RECOMMENDATIONS',\n",
    "    schema='JOB_DATA',\n",
    "    role='ACCOUNTADMIN'\n",
    ")\n",
    "print(\"Connected to Snowflake successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1bcd27f0-2924-49d9-b60f-3c5f3626d060",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved 1615940 job postings.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Job Id</th>\n",
       "      <th>Experience</th>\n",
       "      <th>Qualifications</th>\n",
       "      <th>Salary Range</th>\n",
       "      <th>location</th>\n",
       "      <th>Country</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>Work Type</th>\n",
       "      <th>Company Size</th>\n",
       "      <th>...</th>\n",
       "      <th>Contact</th>\n",
       "      <th>Job Title</th>\n",
       "      <th>Role</th>\n",
       "      <th>Job Portal</th>\n",
       "      <th>Job Description</th>\n",
       "      <th>Benefits</th>\n",
       "      <th>skills</th>\n",
       "      <th>Responsibilities</th>\n",
       "      <th>Company</th>\n",
       "      <th>Company Profile</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2600342200917599</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>$61K-$106K</td>\n",
       "      <td>Capitol Hill, Saipan</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>Purchasing Agent</td>\n",
       "      <td>Inventory Manager</td>\n",
       "      <td>None</td>\n",
       "      <td>An Inventory Manager oversees inventory levels...</td>\n",
       "      <td>None</td>\n",
       "      <td>Inventory control Demand forecasting Supply ch...</td>\n",
       "      <td>None</td>\n",
       "      <td>Kyndryl Holdings</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1097571695278272</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>$57K-$86K</td>\n",
       "      <td>Banjul</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>Graphic Designer</td>\n",
       "      <td>Web Graphic Designer</td>\n",
       "      <td>None</td>\n",
       "      <td>Web Graphic Designers create visually appealin...</td>\n",
       "      <td>None</td>\n",
       "      <td>Graphic design tools (e.g., Adobe Creative Sui...</td>\n",
       "      <td>None</td>\n",
       "      <td>Ambuja Cements</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>393705790719989</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>$60K-$103K</td>\n",
       "      <td>Tashkent</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>Physician Assistant</td>\n",
       "      <td>Surgical Physician Assistant</td>\n",
       "      <td>None</td>\n",
       "      <td>Assist surgeons in the operating room, perform...</td>\n",
       "      <td>None</td>\n",
       "      <td>Surgical procedures and techniques Operating r...</td>\n",
       "      <td>None</td>\n",
       "      <td>Whitehaven Coal</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Job Id Experience Qualifications Salary Range  \\\n",
       "0  2600342200917599       None           None   $61K-$106K   \n",
       "1  1097571695278272       None           None    $57K-$86K   \n",
       "2   393705790719989       None           None   $60K-$103K   \n",
       "\n",
       "               location Country  latitude  longitude Work Type  Company Size  \\\n",
       "0  Capitol Hill, Saipan    None       NaN        NaN      None           NaN   \n",
       "1                Banjul    None       NaN        NaN      None           NaN   \n",
       "2              Tashkent    None       NaN        NaN      None           NaN   \n",
       "\n",
       "   ... Contact            Job Title                          Role Job Portal  \\\n",
       "0  ...    None     Purchasing Agent             Inventory Manager       None   \n",
       "1  ...    None     Graphic Designer          Web Graphic Designer       None   \n",
       "2  ...    None  Physician Assistant  Surgical Physician Assistant       None   \n",
       "\n",
       "                                     Job Description Benefits  \\\n",
       "0  An Inventory Manager oversees inventory levels...     None   \n",
       "1  Web Graphic Designers create visually appealin...     None   \n",
       "2  Assist surgeons in the operating room, perform...     None   \n",
       "\n",
       "                                              skills Responsibilities  \\\n",
       "0  Inventory control Demand forecasting Supply ch...             None   \n",
       "1  Graphic design tools (e.g., Adobe Creative Sui...             None   \n",
       "2  Surgical procedures and techniques Operating r...             None   \n",
       "\n",
       "            Company Company Profile  \n",
       "0  Kyndryl Holdings            None  \n",
       "1    Ambuja Cements            None  \n",
       "2   Whitehaven Coal            None  \n",
       "\n",
       "[3 rows x 23 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Execute a query to retrieve all records from the JOBS table\n",
    "query = \"SELECT * FROM JOB_DESC;\"\n",
    "cur = conn.cursor()\n",
    "cur.execute(query)\n",
    "\n",
    "# Fetch all results into a pandas DataFrame\n",
    "jobs_df = cur.fetch_pandas_all()  # Loads all rows into a DataFrame&#8203;:contentReference[oaicite:1]{index=1}\n",
    "print(f\"Retrieved {len(jobs_df)} job postings.\")\n",
    "jobs_df.head(3)  # display first few rows for verification (optional)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a0e6b8dd-b299-483e-9680-56c1e40d7362",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Job Recommender LSH Started ---\n",
      "Checking cache status...\n",
      "\n",
      "Cache Status:\n",
      "  ✓ LSH Index: 703.77 MB (Last modified: 2025-04-23 20:35:23)\n",
      "  ✓ MinHashes: 802.49 MB (Last modified: 2025-04-23 20:38:16)\n",
      "  ✓ Jobs DataFrame: 413.39 MB (Last modified: 2025-04-23 20:38:36)\n",
      "\n",
      "Extracting text from resume: sampleresume.pdf\n",
      "Building or loading LSH index (Force rebuild: False)\n",
      "CACHE HIT: Loading cached LSH index from /Users/likhithgunjal/Documents/newest project/lsh_cache\n",
      "Cache loaded in 246.37 seconds\n",
      "DataFrame structure matches cache. Using cached data.\n",
      "Finding matches for resume...\n",
      "Processing resume text...\n",
      "Finding matches in LSH index...\n",
      "Found 3547 potential matches. Calculating similarities...\n",
      "\n",
      "--- Job Recommender completed in 246.46 seconds ---\n",
      "\n",
      "Top job matches:\n",
      "                    Job Title                   Company                  Role  \\\n",
      "399355   Social Media Manager  Community Health Systems  Social Media Analyst   \n",
      "837255   Social Media Manager  Community Health Systems  Social Media Analyst   \n",
      "1613744  Social Media Manager  Community Health Systems  Social Media Analyst   \n",
      "366029   Social Media Manager  Community Health Systems  Social Media Analyst   \n",
      "1351549  Social Media Manager  Community Health Systems  Social Media Analyst   \n",
      "\n",
      "                 location  similarity  \n",
      "399355   Charlotte Amalie    0.140625  \n",
      "837255             Kigali    0.140625  \n",
      "1613744           Thimphu    0.140625  \n",
      "366029           Monrovia    0.140625  \n",
      "1351549      Kuala Lumpur    0.140625  \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import string\n",
    "import time\n",
    "import logging\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "from datasketch import MinHash, MinHashLSH\n",
    "\n",
    "try:\n",
    "    import fitz  # PyMuPDF for faster PDF processing\n",
    "except ImportError:\n",
    "    raise ImportError(\"The 'fitz' module (PyMuPDF) is not installed. Install it via 'pip install PyMuPDF'\")\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(message)s', filename='job_recommender_lsh.log')\n",
    "logger = logging.getLogger()\n",
    "\n",
    "# --- Cache Status Verification ---\n",
    "\n",
    "def check_cache_status(cache_dir=\"./lsh_cache\"):\n",
    "    \"\"\"Check and display cache file status\"\"\"\n",
    "    if not os.path.exists(cache_dir):\n",
    "        print(f\"Cache directory {cache_dir} does not exist!\")\n",
    "        return False\n",
    "        \n",
    "    cache_files = {\n",
    "        \"LSH Index\": os.path.join(cache_dir, \"lsh_index.pkl\"),\n",
    "        \"MinHashes\": os.path.join(cache_dir, \"minhashes.pkl\"),\n",
    "        \"Jobs DataFrame\": os.path.join(cache_dir, \"processed_jobs_df.pkl\")\n",
    "    }\n",
    "    \n",
    "    print(\"\\nCache Status:\")\n",
    "    all_exist = True\n",
    "    for name, path in cache_files.items():\n",
    "        if os.path.exists(path):\n",
    "            size_mb = os.path.getsize(path) / (1024 * 1024)\n",
    "            mod_time = time.strftime('%Y-%m-%d %H:%M:%S', \n",
    "                       time.localtime(os.path.getmtime(path)))\n",
    "            print(f\"  ✓ {name}: {size_mb:.2f} MB (Last modified: {mod_time})\")\n",
    "        else:\n",
    "            print(f\"  ✗ {name}: Not found\")\n",
    "            all_exist = False\n",
    "    print()\n",
    "    return all_exist\n",
    "\n",
    "# --- Utility Functions ---\n",
    "\n",
    "def extract_resume_text(pdf_path):\n",
    "    try:\n",
    "        doc = fitz.open(pdf_path)\n",
    "        text = \" \".join([page.get_text() for page in doc])\n",
    "        doc.close()\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        logger.error(f\"PDF extraction failed: {e}\")\n",
    "        print(f\"Error extracting text from {pdf_path}: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "def clean_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\n', ' ', text)\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    return re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "def weight_text(text, weight):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    weighted = text * int(weight)\n",
    "    frac = weight - int(weight)\n",
    "    if frac:\n",
    "        words = text.split()\n",
    "        weighted += \" \" + \" \".join(words[:int(len(words) * frac)])\n",
    "    return weighted\n",
    "\n",
    "def get_shingles(text, k=5):\n",
    "    if len(text) < k:\n",
    "        return set()\n",
    "    return set([text[i:i+k] for i in range(len(text)-k+1)])\n",
    "\n",
    "# --- MinHash LSH Embedding with Caching ---\n",
    "\n",
    "def build_or_load_lsh_index(jobs_df, cache_dir=\"./lsh_cache\", num_perm=128, force_rebuild=False):\n",
    "    os.makedirs(cache_dir, exist_ok=True)\n",
    "    lsh_path = os.path.join(cache_dir, \"lsh_index.pkl\")\n",
    "    minhash_path = os.path.join(cache_dir, \"minhashes.pkl\")\n",
    "    df_path = os.path.join(cache_dir, \"processed_jobs_df.pkl\")\n",
    "\n",
    "    # Check if all cache files exist\n",
    "    cache_exists = all(os.path.exists(path) for path in [lsh_path, minhash_path, df_path])\n",
    "    \n",
    "    if cache_exists and not force_rebuild:\n",
    "        print(f\"CACHE HIT: Loading cached LSH index from {os.path.abspath(cache_dir)}\")\n",
    "        logger.info(\"Loading cached LSH index and MinHashes...\")\n",
    "        start_time = time.time()\n",
    "        try:\n",
    "            lsh = joblib.load(lsh_path)\n",
    "            minhashes = joblib.load(minhash_path)\n",
    "            cached_df = joblib.load(df_path)\n",
    "            load_time = time.time() - start_time\n",
    "            print(f\"Cache loaded in {load_time:.2f} seconds\")\n",
    "            \n",
    "            # Verify if cached DataFrame structure matches the current one\n",
    "            if list(cached_df.columns) == list(jobs_df.columns) and len(cached_df) == len(jobs_df):\n",
    "                print(\"DataFrame structure matches cache. Using cached data.\")\n",
    "                return lsh, minhashes, cached_df\n",
    "            else:\n",
    "                print(\"WARNING: DataFrame structure has changed. Rebuilding cache...\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading cache: {e}. Rebuilding...\")\n",
    "    else:\n",
    "        if force_rebuild:\n",
    "            print(\"Force rebuilding LSH index as requested.\")\n",
    "        else:\n",
    "            print(f\"CACHE MISS: Building new LSH index (files not found in {os.path.abspath(cache_dir)})\")\n",
    "        \n",
    "    logger.info(\"Building new LSH index...\")\n",
    "    print(\"Building LSH index from scratch. This may take some time...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Process text with weights\n",
    "    weights = {\n",
    "        'Job Title': 3.0, 'Role': 2.5, 'skills': 2.0, 'Job Description': 1.0, 'Company': 0.8\n",
    "    }\n",
    "    \n",
    "    jobs_df['Weighted_Text'] = \"\"\n",
    "    for field, weight in weights.items():\n",
    "        if field in jobs_df.columns:\n",
    "            jobs_df['Weighted_Text'] += jobs_df[field].fillna('').apply(lambda x: weight_text(str(x), weight) + \" \")\n",
    "\n",
    "    jobs_df['Weighted_Text_Clean'] = jobs_df['Weighted_Text'].apply(clean_text)\n",
    "\n",
    "    lsh = MinHashLSH(threshold=0.3, num_perm=num_perm)\n",
    "    minhashes = {}\n",
    "\n",
    "    total_rows = len(jobs_df)\n",
    "    for i, (idx, row) in enumerate(jobs_df.iterrows()):\n",
    "        if i % 100 == 0:\n",
    "            print(f\"Processing {i}/{total_rows} jobs ({i/total_rows*100:.1f}%)...\")\n",
    "            \n",
    "        shingles = get_shingles(row['Weighted_Text_Clean'])\n",
    "        m = MinHash(num_perm=num_perm)\n",
    "        for shingle in shingles:\n",
    "            m.update(shingle.encode('utf8'))\n",
    "        lsh.insert(str(idx), m)\n",
    "        minhashes[str(idx)] = m\n",
    "\n",
    "    build_time = time.time() - start_time\n",
    "    print(f\"LSH index built in {build_time:.2f} seconds\")\n",
    "    \n",
    "    # Save to cache\n",
    "    print(\"Saving LSH index to cache...\")\n",
    "    try:\n",
    "        joblib.dump(lsh, lsh_path, compress=3)\n",
    "        joblib.dump(minhashes, minhash_path, compress=3)\n",
    "        joblib.dump(jobs_df, df_path, compress=3)\n",
    "        print(f\"Cache saved to {os.path.abspath(cache_dir)}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Failed to save cache: {e}\")\n",
    "    \n",
    "    return lsh, minhashes, jobs_df\n",
    "\n",
    "# --- Similarity Matching ---\n",
    "\n",
    "def match_resume_lsh(resume_text, lsh, minhashes, jobs_df, num_perm=128, top_n=10):\n",
    "    print(\"Processing resume text...\")\n",
    "    \n",
    "    resume_clean = clean_text(resume_text)\n",
    "    shingles = get_shingles(resume_clean)\n",
    "    \n",
    "    if not shingles:\n",
    "        print(\"Warning: No shingles generated from resume. Text might be too short or empty.\")\n",
    "        return pd.DataFrame({'Message': ['No valid content found in resume']})\n",
    "        \n",
    "    m = MinHash(num_perm=num_perm)\n",
    "    for sh in shingles:\n",
    "        m.update(sh.encode('utf8'))\n",
    "\n",
    "    print(\"Finding matches in LSH index...\")\n",
    "    result_ids = lsh.query(m)\n",
    "    \n",
    "    if not result_ids:\n",
    "        print(\"No matches found in LSH. Try lowering the threshold.\")\n",
    "        return pd.DataFrame({'Message': ['No matches found']})\n",
    "        \n",
    "    print(f\"Found {len(result_ids)} potential matches. Calculating similarities...\")\n",
    "    \n",
    "    similarities = []\n",
    "    for idx in result_ids:\n",
    "        jaccard_sim = m.jaccard(minhashes[idx])\n",
    "        similarities.append((int(idx), jaccard_sim))\n",
    "\n",
    "    similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "    top_ids = [idx for idx, _ in similarities[:top_n]]\n",
    "    top_scores = [score for _, score in similarities[:top_n]]\n",
    "\n",
    "    result_df = jobs_df.loc[top_ids].copy()\n",
    "    result_df['similarity'] = top_scores\n",
    "    \n",
    "    return result_df\n",
    "\n",
    "# --- Main Recommender ---\n",
    "\n",
    "def run_job_recommender_lsh(resume_path, jobs_df, cache_dir=\"./lsh_cache\", top_n=10, force_rebuild=False, num_perm=128):\n",
    "    print(f\"\\n--- Job Recommender LSH Started ---\")\n",
    "    start = time.time()\n",
    "    \n",
    "    # Check cache status\n",
    "    print(\"Checking cache status...\")\n",
    "    cache_exists = check_cache_status(cache_dir)\n",
    "    \n",
    "    print(f\"Extracting text from resume: {resume_path}\")\n",
    "    resume_text = extract_resume_text(resume_path)\n",
    "    \n",
    "    if not resume_text:\n",
    "        logger.warning(\"Resume extraction failed.\")\n",
    "        return pd.DataFrame({'Message': ['Resume extraction failed']})\n",
    "\n",
    "    print(f\"Building or loading LSH index (Force rebuild: {force_rebuild})\")\n",
    "    lsh, minhashes, processed_df = build_or_load_lsh_index(\n",
    "        jobs_df, \n",
    "        cache_dir=cache_dir, \n",
    "        num_perm=num_perm,\n",
    "        force_rebuild=force_rebuild\n",
    "    )\n",
    "    \n",
    "    print(f\"Finding matches for resume...\")\n",
    "    results = match_resume_lsh(resume_text, lsh, minhashes, processed_df, top_n=top_n, num_perm=num_perm)\n",
    "\n",
    "    total_time = time.time() - start\n",
    "    print(f\"\\n--- Job Recommender completed in {total_time:.2f} seconds ---\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# --- Example Execution ---\n",
    "if __name__ == \"__main__\":\n",
    "    # Replace with your actual jobs DataFrame\n",
    "    # For example:\n",
    "    # jobs_df = pd.read_csv('jobs_data.csv')\n",
    "    \n",
    "    # Sample data if jobs_df is not defined\n",
    "    try:\n",
    "        if 'jobs_df' not in globals():\n",
    "            print(\"Creating sample jobs dataframe for demonstration\")\n",
    "            jobs_df = pd.DataFrame({\n",
    "                'Job Title': ['Data Scientist', 'Software Engineer', 'Product Manager'],\n",
    "                'Company': ['Tech Co', 'Software Inc', 'Product Corp'],\n",
    "                'Role': ['Analytics', 'Development', 'Management'],\n",
    "                'skills': ['Python, SQL, ML', 'Java, C++, Python', 'Agile, Leadership'],\n",
    "                'Job Description': ['Analyze data...', 'Build software...', 'Manage products...'],\n",
    "                'location': ['New York', 'San Francisco', 'Chicago']\n",
    "            })\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating sample data: {e}\")\n",
    "        \n",
    "    resume_path = 'sampleresume.pdf'  # Replace with your resume path\n",
    "    \n",
    "    # Optional: Force rebuild cache if needed\n",
    "    force_rebuild = False  # Set to True to force rebuild\n",
    "    \n",
    "    # Run the recommender\n",
    "    top_matches = run_job_recommender_lsh(\n",
    "        resume_path=resume_path,\n",
    "        jobs_df=jobs_df,\n",
    "        top_n=5,\n",
    "        force_rebuild=force_rebuild,\n",
    "        cache_dir=\"./lsh_cache\",  # Specify cache directory explicitly\n",
    "        num_perm=128  # Reduce to 64 for faster processing if needed\n",
    "    )\n",
    "    \n",
    "    # Display results\n",
    "    if 'Message' in top_matches.columns:\n",
    "        print(\"\\nResult:\", top_matches['Message'].values[0])\n",
    "    else:\n",
    "        print(\"\\nTop job matches:\")\n",
    "        print(top_matches[['Job Title', 'Company', 'Role', 'location', 'similarity']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcfba155-55f8-4be8-ac5a-befbcc2163b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
